{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# A basic `TensorFlow` installation check\n",
    "\n",
    "Pre-reqs: If nothing is working right, check this notebook, [../jims_ml_notebook_env.py].\n",
    "\n",
    "A sanity check call to `tensorflow` to make sure it finds and loads the library. NOTE: this does not do any tensor processing:"
   ],
   "id": "2a9bc4e2ec05f5db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T21:44:38.566952Z",
     "start_time": "2025-03-12T21:44:35.291105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"A TF computation: {tf.add(1, 5).numpy()}\")\n",
    "\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(f\"A TensorFlow string constant: {hello.numpy()}\")"
   ],
   "id": "3814a818a6d07176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A TF computation: 6\n",
      "A TensorFlow string constant: b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Of special `import` for `keras`\n",
    "Keras notions should be imported from `keras.*`, not from `tensorflow`.\n",
    "\n",
    "When coding to the conventional API:"
   ],
   "id": "1315965ff70582e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T21:44:38.573652Z",
     "start_time": "2025-03-12T21:44:38.571570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Imports when using the conventional class-centric Keras API\n",
    "\n",
    "# from keras.api.datasets import mnist\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Input, Dense\n",
    "\n"
   ],
   "id": "d05cb1fe7133365a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "When coding to the newer functional Keras API:\n"
   ],
   "id": "c604df980cf7df0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T21:44:38.680888Z",
     "start_time": "2025-03-12T21:44:38.679405Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "7fad8da6c28f3ff6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we load a dataset from the convenient tensorflow_datasets python package.",
   "id": "3d85093ca3d9c35d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T21:44:42.110263Z",
     "start_time": "2025-03-12T21:44:38.686730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import jkcsoft.ml.tensorflow_utils\n",
    "\n",
    "importlib.reload(jkcsoft.ml.tensorflow_utils)\n",
    "\n",
    "from jkcsoft.ml.tensorflow_utils import dump_dataset_info, display_dataset_info\n",
    "\n",
    "# Validates datasets package install and lists voluminous available datasets\n",
    "dump_dataset_info()\n",
    "\n",
    "# Create an array of 5 most popular datasets\n",
    "most_popular_datasets = ['mnist', 'cifar10', 'imdb_reviews', 'fashion_mnist', 'coco']\n",
    "print(f\"Most popular datasets: {most_popular_datasets}\")\n",
    "\n",
    "for dataset_name in most_popular_datasets:\n",
    "    display_dataset_info(dataset_name)\n"
   ],
   "id": "33d3dd51918dda5e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:44:39.106353: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n",
      "WARNING:absl:You use TensorFlow DType <dtype: 'int64'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int64.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total datasets available: 1303\n",
      "A few available datasets: ['abstract_reasoning', 'accentdb', 'aeslc', 'aflw2k3d', 'ag_news_subset', 'ai2_arc', 'ai2_arc_with_ir', 'ai2dcaption', 'aloha_mobile', 'amazon_us_reviews'] ...\n",
      "Most popular datasets: ['mnist', 'cifar10', 'imdb_reviews', 'fashion_mnist', 'coco']\n",
      "\n",
      "Dataset: mnist\n",
      "Description: The MNIST database of handwritten digits.\n",
      "Features: FeaturesDict({\n",
      "    'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
      "    'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
      "})\n",
      "Supervised keys: ('image', 'label')\n",
      "Splits: {'test': <SplitInfo num_examples=10000, num_shards=1>, 'train': <SplitInfo num_examples=60000, num_shards=1>}\n",
      "Dataset size: 21.00 MiB bytes\n",
      "Citation: @article{lecun2010mnist,\n",
      "  title={MNIST handwritten digit database},\n",
      "  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
      "  volume={2},\n",
      "  year={2010}\n",
      "}\n",
      "\n",
      "Dataset: cifar10\n",
      "Description: The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
      "Features: FeaturesDict({\n",
      "    'id': Text(shape=(), dtype=string),\n",
      "    'image': Image(shape=(32, 32, 3), dtype=uint8),\n",
      "    'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
      "})\n",
      "Supervised keys: ('image', 'label')\n",
      "Splits: {'train': <SplitInfo num_examples=50000, num_shards=1>, 'test': <SplitInfo num_examples=10000, num_shards=1>}\n",
      "Dataset size: 132.40 MiB bytes\n",
      "Citation: @TECHREPORT{Krizhevsky09learningmultiple,\n",
      "    author = {Alex Krizhevsky},\n",
      "    title = {Learning multiple layers of features from tiny images},\n",
      "    institution = {},\n",
      "    year = {2009}\n",
      "}\n",
      "\n",
      "Dataset: imdb_reviews\n",
      "Description: Large Movie Review Dataset. This is a dataset for binary sentiment\n",
      "classification containing substantially more data than previous benchmark\n",
      "datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
      "and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "Features: FeaturesDict({\n",
      "    'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "    'text': Text(shape=(), dtype=string),\n",
      "})\n",
      "Supervised keys: ('text', 'label')\n",
      "Splits: {'train': <SplitInfo num_examples=25000, num_shards=1>, 'test': <SplitInfo num_examples=25000, num_shards=1>, 'unsupervised': <SplitInfo num_examples=50000, num_shards=1>}\n",
      "Dataset size: 129.83 MiB bytes\n",
      "Citation: @InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "Dataset: fashion_mnist\n",
      "Description: Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
      "Features: FeaturesDict({\n",
      "    'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
      "    'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
      "})\n",
      "Supervised keys: ('image', 'label')\n",
      "Splits: {'train': <SplitInfo num_examples=60000, num_shards=1>, 'test': <SplitInfo num_examples=10000, num_shards=1>}\n",
      "Dataset size: 36.42 MiB bytes\n",
      "Citation: @article{DBLP:journals/corr/abs-1708-07747,\n",
      "  author    = {Han Xiao and\n",
      "               Kashif Rasul and\n",
      "               Roland Vollgraf},\n",
      "  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n",
      "               Algorithms},\n",
      "  journal   = {CoRR},\n",
      "  volume    = {abs/1708.07747},\n",
      "  year      = {2017},\n",
      "  url       = {http://arxiv.org/abs/1708.07747},\n",
      "  archivePrefix = {arXiv},\n",
      "  eprint    = {1708.07747},\n",
      "  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},\n",
      "  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},\n",
      "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
      "}\n",
      "\n",
      "Dataset: coco\n",
      "Description: COCO is a large-scale object detection, segmentation, and\n",
      "captioning dataset.\n",
      "\n",
      "Note:\n",
      " * Some images from the train and validation sets don't have annotations.\n",
      " * Coco 2014 and 2017 uses the same images, but different train/val/test splits\n",
      " * The test split don't have any annotations (only images).\n",
      " * Coco defines 91 classes but the data only uses 80 classes.\n",
      " * Panotptic annotations defines defines 200 classes but only uses 133.\n",
      "Features: FeaturesDict({\n",
      "    'image': Image(shape=(None, None, 3), dtype=uint8),\n",
      "    'image/filename': Text(shape=(), dtype=string),\n",
      "    'image/id': int64,\n",
      "    'objects': Sequence({\n",
      "        'area': int64,\n",
      "        'bbox': BBoxFeature(shape=(4,), dtype=float32),\n",
      "        'id': int64,\n",
      "        'is_crowd': bool,\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=80),\n",
      "    }),\n",
      "})\n",
      "Supervised keys: None\n",
      "Splits: {'test': <SplitInfo num_examples=40775, num_shards=64>, 'test2015': <SplitInfo num_examples=81434, num_shards=128>, 'train': <SplitInfo num_examples=82783, num_shards=128>, 'validation': <SplitInfo num_examples=40504, num_shards=64>}\n",
      "Dataset size: Unknown size bytes\n",
      "Citation: @article{DBLP:journals/corr/LinMBHPRDZ14,\n",
      "  author    = {Tsung{-}Yi Lin and\n",
      "               Michael Maire and\n",
      "               Serge J. Belongie and\n",
      "               Lubomir D. Bourdev and\n",
      "               Ross B. Girshick and\n",
      "               James Hays and\n",
      "               Pietro Perona and\n",
      "               Deva Ramanan and\n",
      "               Piotr Doll{'{a}}r and\n",
      "               C. Lawrence Zitnick},\n",
      "  title     = {Microsoft {COCO:} Common Objects in Context},\n",
      "  journal   = {CoRR},\n",
      "  volume    = {abs/1405.0312},\n",
      "  year      = {2014},\n",
      "  url       = {http://arxiv.org/abs/1405.0312},\n",
      "  archivePrefix = {arXiv},\n",
      "  eprint    = {1405.0312},\n",
      "  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},\n",
      "  biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},\n",
      "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setup training and test datums to be used in the next two examples\n",
    "\n",
    "The common datums are used to compare the use of the two Keras APIs.\n"
   ],
   "id": "7174123b6113b98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T21:44:42.300741Z",
     "start_time": "2025-03-12T21:44:42.119751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "from jkcsoft.ml.test_results_db.ml_results_database_orm import ModelTestRun, RunBatch, TestDataset\n",
    "\n",
    "# define a batch include a common dataset for all tests\n",
    "run_batch = RunBatch()\n",
    "nb_name = \"jims_ml_tensorflow.ipynb\"\n",
    "dataset = TestDataset(f\"adhoc {nb_name}\",\n",
    "                      x_train=np.array([[1.0], [2.0], [3.0], [4.0]]),\n",
    "                      y_train=np.array([[2.0], [4.0], [6.0], [8.0]]),\n",
    "                      x_test=np.array([[5.0], [6.0]]),\n",
    "                      y_test=np.array([[10.0], [12.0]])\n",
    "                      )\n",
    "\n",
    "dataset"
   ],
   "id": "df1acb5bb736369",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestDataset(name='adhoc jims_ml_tensorflow.ipynb', x_train shape=(4, 1), y_train shape=(4, 1), x_test shape=(2, 1), y_test shape=(2, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# A simple _conventional_ Keras pipeline with hard-coded TensorFlow datasets",
   "id": "9c6e7b2dc9d3b8ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T20:27:22.927397Z",
     "start_time": "2025-03-12T20:27:22.625305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a model using the conventional Keras class API\n",
    "model = Sequential([\n",
    "    Input(shape=(1,)),  # Input layer for single-dimensional input\n",
    "    Dense(32, activation=\"relu\"),  # kera.src.core.dense.Dense\n",
    "    Dense(16, activation=\"sigmoid\"),\n",
    "    Dense(16, activation=\"softmax\"),\n",
    "    Dense(1, activation=\"linear\")\n",
    "])\n",
    "\n",
    "test_common_dense = ModelTestRun(TestDataset(\"\"), model)\n",
    "test_common_dense.description = \"Common Dense Sequence\"\n",
    "test_common_dense.fit_epochs(10)\n",
    "test_common_dense.backend = \"tensorflow\"\n",
    "test_common_dense.frontend = \"keras\"\n",
    "test_common_dense.backend_proc = \"cpu\"\n",
    "\n",
    "# invoke model compile here\n",
    "test_common_dense.compile_model(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\n",
    "        \"mean_absolute_error\",\n",
    "        \"mean_absolute_percentage_error\",\n",
    "        \"mean_squared_error\",\n",
    "        \"root_mean_squared_error\",\n",
    "        \"accuracy\",\n",
    "        \"precision\",\n",
    "    ])\n",
    "\n",
    "# test_common_dense.model.fit()\n",
    "\n",
    "run_batch.add_test(test_common_dense)\n",
    "\n",
    "# run all models against the same batch dataset\n",
    "run_batch.run_all()\n",
    "\n",
    "print('\\nTest accuracy:', run_batch.results[0].eval_results)"
   ],
   "id": "2e98b868ec73286d",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelTestRun' object has no attribute 'set_description'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 11\u001B[0m\n\u001B[1;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m Sequential([\n\u001B[1;32m      3\u001B[0m     Input(shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m,)),  \u001B[38;5;66;03m# Input layer for single-dimensional input\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     Dense(\u001B[38;5;241m32\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m\"\u001B[39m),  \u001B[38;5;66;03m# kera.src.core.dense.Dense\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     Dense(\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlinear\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m ])\n\u001B[1;32m     10\u001B[0m test_common_dense \u001B[38;5;241m=\u001B[39m ModelTestRun(TestDataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m), model)\n\u001B[0;32m---> 11\u001B[0m \u001B[43mtest_common_dense\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_description\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCommon Dense Sequence\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mset_model(model)\n\u001B[1;32m     12\u001B[0m test_common_dense\u001B[38;5;241m.\u001B[39mset_fit_epochs(\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# invoke model compile here\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ModelTestRun' object has no attribute 'set_description'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# A simple pipeline with the Keras _functional_ API",
   "id": "422842e3e21bc460"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Keep the input tensor\n",
    "inputs_fn = keras.Input(shape=(1,))\n",
    "\n",
    "print(f\"inputs shape: {inputs_fn.shape}\")\n",
    "\n",
    "print(f\"inputs dtype: {inputs_fn.dtype}\")\n",
    "\n",
    "# pipe input\n",
    "d1 = layers.Dense(32, activation=\"relu\")\n",
    "x = d1(inputs_fn)\n",
    "\n",
    "# pipe thru proc layers\n",
    "d2 = layers.Dense(16, activation=\"sigmoid\")\n",
    "x = d2(x)\n",
    "d3 = layers.Dense(8, activation=\"softmax\")\n",
    "x = d3(x)\n",
    "\n",
    "# use final layer to reduce to output\n",
    "final_layer = layers.Dense(1, activation=\"linear\")\n",
    "outputs_fn = final_layer(x)\n",
    "\n",
    "functional_model = keras.Model(inputs=inputs_fn, outputs=outputs_fn, name=\"functional_model_1\")\n",
    "\n",
    "print(f\"functional_model summary: {functional_model.summary()}\")\n",
    "\n",
    "functional_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\n",
    "        \"mean_absolute_error\",\n",
    "        \"mean_absolute_percentage_error\",\n",
    "        \"mean_squared_error\",\n",
    "        \"root_mean_squared_error\",\n",
    "        \"accuracy\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"auc\",\n",
    "        \"binary_accuracy\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "functional_model.fit(run_batch.x_train, run_batch.y_train, epochs=3, verbose=1)\n",
    "\n",
    "loss_and_metrics = functional_model.evaluate(run_batch.x_test, run_batch.y_test, verbose=2)\n",
    "\n",
    "print(f\"Test eval results: {loss_and_metrics}\")"
   ],
   "id": "e7019ccccaa94659",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## A more readable functional pipeline declaration",
   "id": "e478301b84a30d91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jkcsoft.ml import keras_utils\n",
    "\n",
    "importlib.reload(keras_utils)\n",
    "\n",
    "proc_layers = [d1, d2, d3, final_layer]\n",
    "\n",
    "composite_model = keras_utils.compose_model(inputs_fn, proc_layers, verbose=False)\n",
    "\n",
    "print(f\"composite_model summary: {composite_model.summary()}\")\n",
    "\n",
    "composite_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\n",
    "        #        \"mean_absolute_error\",\n",
    "        \"mean_absolute_percentage_error\",\n",
    "        #        \"mean_squared_error\",\n",
    "        #        \"root_mean_squared_error\",\n",
    "        #        \"accuracy\",\n",
    "        #        \"precision\",\n",
    "        #        \"recall\",\n",
    "        #        \"auc\",\n",
    "        #        \"binary_accuracy\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "composite_model.fit(run_batch.x_train, run_batch.y_train, epochs=3, verbose=1)\n",
    "\n",
    "loss_and_metrics = composite_model.evaluate(run_batch.x_test, run_batch.y_test, verbose=2)\n",
    "\n",
    "print(f\"Test eval results: {loss_and_metrics}\")"
   ],
   "id": "afd9f15c2ad0b8b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## A _functional_ model loaded into our `RunBatch` apparatus",
   "id": "bbee7ecdf45019e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "test_common_dense_fn = ModelTestRun().set_description(\"Common Dense Sequence\").set_model(functional_model)\n",
    "test_common_dense_fn.set_fit_epochs(10)\n",
    "test_common_dense_fn.compile_model(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\n",
    "        #        \"mean_absolute_error\",\n",
    "        \"mean_absolute_percentage_error\",\n",
    "        #        \"mean_squared_error\",\n",
    "        #        \"root_mean_squared_error\",\n",
    "        #        \"accuracy\",\n",
    "        #        \"precision\",\n",
    "        #        \"recall\",\n",
    "        #        \"auc\",\n",
    "        #        \"binary_accuracy\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_common_dense_fn.model.summary()\n",
    "\n",
    "test_common_dense_fn.model.fit(run_batch.x_train, run_batch.y_train, epochs=10, verbose=1)\n",
    "\n",
    "run_batch.add_test(test_common_dense_fn)\n",
    "\n",
    "run_batch.run_all()\n",
    "\n",
    "for i, result in enumerate(run_batch.results):\n",
    "    print(f\"Test {i} eval results: {result.eval_results}\")\n"
   ],
   "id": "448674ee8410b86e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
