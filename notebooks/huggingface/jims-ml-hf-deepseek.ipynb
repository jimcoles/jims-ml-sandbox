{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using a more complex HF model: `DeepSeek-R1`",
   "id": "83edb4ef57e02957"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:20:22.571529Z",
     "start_time": "2025-02-16T22:20:22.012276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoConfig\n",
    "import optimum\n",
    "\n",
    "print(\"Optimum version:\", optimum)\n",
    "\n",
    "DEEP_SEEK_MODEL_KEY = \"deepseek-ai/DeepSeek-R1\"\n",
    "\n",
    "# Use trust_remote_code to allow loading custom configuration and model from the repository\n",
    "config = AutoConfig.from_pretrained(DEEP_SEEK_MODEL_KEY, trust_remote_code=True)\n",
    "\n",
    "# Override because default of \"fp8\" is not recognized by\n",
    "# config.quantization_config[\"quant_method\"] = \"bitsandbytes_8bit\"\n",
    "# config.quantization_config[\"quant_method\"] = \"default\"\n",
    "# config.quantization_config[\"quant_method\"] = \"gptq\"\n",
    "# config.quantization_config = {\n",
    "#     \"quant_method\": \"gptq\",\n",
    "#     \"bits\": 4,  # Define the quantization bit-width, e.g., 4 bits\n",
    "#     \"group_size\": -1,  # Example value; adjust for your model requirements\n",
    "#     # You can add other optional parameters here depending on GPTQConfig\n",
    "# }\n",
    "\n",
    "\n",
    "# 'awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'higgs', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet', 'vptq'\n",
    "\n",
    "config.quantization_config = {\n",
    "#    \"quant_method\": \"default\",\n",
    "#    \"quant_method\": \"bitsandbytes_8bit\",\n",
    "    \"quant_method\": \"aqlm\",\n",
    "    \"bits\": 8,  # Define the quantization bit-width, e.g., 4 bits\n",
    "    \"group_size\": -1,  # Example value; adjust for your model requirements\n",
    "    # You can add other optional parameters here depending on GPTQConfig\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(DEEP_SEEK_MODEL_KEY,\n",
    "                                             trust_remote_code=True,\n",
    "                                             ignore_mismatched_sizes=True,\n",
    "                                             config=config  # use tweaked config\n",
    "                                             )\n",
    "\n",
    "# Hugging Face pipeline now works with the custom model\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=DEEP_SEEK_MODEL_KEY, trust_remote_code=True)\n",
    "#\n",
    "# Generate text: for DeepSeek\n",
    "output = pipe(\"Generate a sample text using DeepSeek-R1.\",\n",
    "              truncation=True,\n",
    "              max_length=50,\n",
    "              num_return_sequences=1,\n",
    "              temperature=0.6,\n",
    "              #              quantization_type=\"gptq\",\n",
    "              trust_remote_code=True\n",
    "              )\n",
    "\n",
    "#output = pipe(\"Test the custom DeepSeek-R1 model\", max_length=50)\n",
    "\n",
    "print(output)\n"
   ],
   "id": "27b5d3c69ef94415",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum version: <module 'optimum' (<_frozen_importlib_external._NamespaceLoader object at 0x1061658a0>)>\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `aqlm` quantization requires AQLM: `pip install aqlm[gpu,cpu]`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 34\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Override because default of \"fp8\" is not recognized by\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# config.quantization_config[\"quant_method\"] = \"bitsandbytes_8bit\"\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# config.quantization_config[\"quant_method\"] = \"default\"\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     22\u001B[0m \n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# 'awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'higgs', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet', 'vptq'\u001B[39;00m\n\u001B[1;32m     25\u001B[0m config\u001B[38;5;241m.\u001B[39mquantization_config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m#    \"quant_method\": \"default\",\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m#    \"quant_method\": \"bitsandbytes_8bit\",\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;66;03m# You can add other optional parameters here depending on GPTQConfig\u001B[39;00m\n\u001B[1;32m     32\u001B[0m }\n\u001B[0;32m---> 34\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEEP_SEEK_MODEL_KEY\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# use tweaked config\u001B[39;49;00m\n\u001B[1;32m     38\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Hugging Face pipeline now works with the custom model\u001B[39;00m\n\u001B[1;32m     41\u001B[0m pipe \u001B[38;5;241m=\u001B[39m pipeline(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m, model\u001B[38;5;241m=\u001B[39mmodel, tokenizer\u001B[38;5;241m=\u001B[39mDEEP_SEEK_MODEL_KEY, trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Source/jkc/testbeds/jims-ml-sandbox/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:559\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    557\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mregister(config\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m, model_class, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    558\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m add_generation_mixin_to_remote_model(model_class)\n\u001B[0;32m--> 559\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    560\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    561\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n",
      "File \u001B[0;32m~/Source/jkc/testbeds/jims-ml-sandbox/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3620\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3617\u001B[0m     hf_quantizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3619\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 3620\u001B[0m     \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3621\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3622\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3623\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3624\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3625\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3626\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3627\u001B[0m     torch_dtype \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_torch_dtype(torch_dtype)\n\u001B[1;32m   3628\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_device_map(device_map)\n",
      "File \u001B[0;32m~/Source/jkc/testbeds/jims-ml-sandbox/.venv/lib/python3.10/site-packages/transformers/quantizers/quantizer_aqlm.py:54\u001B[0m, in \u001B[0;36mAqlmHfQuantizer.validate_environment\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `aqlm` quantization requires Accelerate: `pip install accelerate`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_aqlm_available():\n\u001B[0;32m---> 54\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `aqlm` quantization requires AQLM: `pip install aqlm[gpu,cpu]`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mImportError\u001B[0m: Using `aqlm` quantization requires AQLM: `pip install aqlm[gpu,cpu]`"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
