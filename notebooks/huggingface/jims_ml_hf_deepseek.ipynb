{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "48e572d92e148772"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using a more complex HF model: `DeepSeek-R1`",
   "id": "894d376e5b3cd743"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoConfig\n",
    "import optimum\n",
    "\n",
    "print(\"Optimum version:\", optimum)\n",
    "\n",
    "DEEP_SEEK_MODEL_KEY = \"deepseek-ai/DeepSeek-R1\"\n",
    "\n",
    "# Use trust_remote_code to allow loading custom configuration and model from the repository\n",
    "config = AutoConfig.from_pretrained(DEEP_SEEK_MODEL_KEY, trust_remote_code=True)\n",
    "\n",
    "# Override because default of \"fp8\" is not recognized by\n",
    "# config.quantization_config[\"quant_method\"] = \"bitsandbytes_8bit\"\n",
    "# config.quantization_config[\"quant_method\"] = \"default\"\n",
    "# config.quantization_config[\"quant_method\"] = \"gptq\"\n",
    "# config.quantization_config = {\n",
    "#     \"quant_method\": \"gptq\",\n",
    "#     \"bits\": 4,  # Define the quantization bit-width, e.g., 4 bits\n",
    "#     \"group_size\": -1,  # Example value; adjust for your model requirements\n",
    "#     # You can add other optional parameters here depending on GPTQConfig\n",
    "# }\n",
    "\n",
    "\n",
    "# 'awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'higgs', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet', 'vptq'\n",
    "\n",
    "config.quantization_config = {\n",
    "#    \"quant_method\": \"default\",\n",
    "    \"quant_method\": \"bitsandbytes_8bit\",\n",
    "#    \"quant_method\": \"aqlm\",\n",
    "    \"bits\": 8,  # Define the quantization bit-width, e.g., 4 bits\n",
    "    \"group_size\": -1,  # Example value; adjust for your model requirements\n",
    "    # You can add other optional parameters here depending on GPTQConfig\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(DEEP_SEEK_MODEL_KEY,\n",
    "                                             trust_remote_code=True,\n",
    "                                             ignore_mismatched_sizes=True,\n",
    "                                             config=config  # use tweaked config\n",
    "                                             )\n",
    "\n",
    "# Hugging Face pipeline now works with the custom model\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=DEEP_SEEK_MODEL_KEY, trust_remote_code=True)\n",
    "#\n",
    "# Generate text: for DeepSeek\n",
    "output = pipe(\"Generate a sample text using DeepSeek-R1.\",\n",
    "              truncation=True,\n",
    "              max_length=50,\n",
    "              num_return_sequences=1,\n",
    "              temperature=0.6,\n",
    "              #              quantization_type=\"gptq\",\n",
    "              trust_remote_code=True\n",
    "              )\n",
    "\n",
    "#output = pipe(\"Test the custom DeepSeek-R1 model\", max_length=50)\n",
    "\n",
    "print(output)\n"
   ],
   "id": "d7c32ba33b707043"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
