{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using a more complex HF model: `DeepSeek-R1`",
   "id": "894d376e5b3cd743"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T18:50:09.621854Z",
     "start_time": "2025-03-05T18:50:06.054999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoConfig\n",
    "import optimum\n",
    "\n",
    "print(\"Optimum version:\", optimum)\n",
    "\n",
    "DEEP_SEEK_MODEL_KEY = \"deepseek-ai/DeepSeek-R1\"\n",
    "\n",
    "# Use trust_remote_code to allow loading custom configuration and model from the repository\n",
    "config = AutoConfig.from_pretrained(DEEP_SEEK_MODEL_KEY, trust_remote_code=True)\n",
    "\n",
    "# Override because default of \"fp8\" is not recognized by\n",
    "# config.quantization_config[\"quant_method\"] = \"bitsandbytes_8bit\"\n",
    "# config.quantization_config[\"quant_method\"] = \"default\"\n",
    "# config.quantization_config[\"quant_method\"] = \"gptq\"\n",
    "# config.quantization_config = {\n",
    "#     \"quant_method\": \"gptq\",\n",
    "#     \"bits\": 4,  # Define the quantization bit-width, e.g., 4 bits\n",
    "#     \"group_size\": -1,  # Example value; adjust for your model requirements\n",
    "#     # You can add other optional parameters here depending on GPTQConfig\n",
    "# }\n",
    "\n",
    "\n",
    "# 'awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'higgs', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet', 'vptq'\n",
    "\n",
    "config.quantization_config = {\n",
    "#    \"quant_method\": \"default\",\n",
    "    \"quant_method\": \"bitsandbytes_8bit\",\n",
    "#    \"quant_method\": \"aqlm\",\n",
    "    \"bits\": 8,  # Define the quantization bit-width, e.g., 4 bits\n",
    "    \"group_size\": -1,  # Example value; adjust for your model requirements\n",
    "    # You can add other optional parameters here depending on GPTQConfig\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(DEEP_SEEK_MODEL_KEY,\n",
    "                                             trust_remote_code=True,\n",
    "                                             ignore_mismatched_sizes=True,\n",
    "                                             config=config  # use tweaked config\n",
    "                                             )\n",
    "\n",
    "# Hugging Face pipeline now works with the custom model\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=DEEP_SEEK_MODEL_KEY, trust_remote_code=True)\n",
    "#\n",
    "# Generate text: for DeepSeek\n",
    "output = pipe(\"Generate a sample text using DeepSeek-R1.\",\n",
    "              truncation=True,\n",
    "              max_length=50,\n",
    "              num_return_sequences=1,\n",
    "              temperature=0.6,\n",
    "              #              quantization_type=\"gptq\",\n",
    "              trust_remote_code=True\n",
    "              )\n",
    "\n",
    "#output = pipe(\"Test the custom DeepSeek-R1 model\", max_length=50)\n",
    "\n",
    "print(output)\n"
   ],
   "id": "d7c32ba33b707043",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum version: <module 'optimum' (<_frozen_importlib_external._NamespaceLoader object at 0x103ed3df0>)>\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 34\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Override because default of \"fp8\" is not recognized by\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# config.quantization_config[\"quant_method\"] = \"bitsandbytes_8bit\"\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# config.quantization_config[\"quant_method\"] = \"default\"\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     22\u001B[0m \n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# 'awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'higgs', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet', 'vptq'\u001B[39;00m\n\u001B[1;32m     25\u001B[0m config\u001B[38;5;241m.\u001B[39mquantization_config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m#    \"quant_method\": \"default\",\u001B[39;00m\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquant_method\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbitsandbytes_8bit\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;66;03m# You can add other optional parameters here depending on GPTQConfig\u001B[39;00m\n\u001B[1;32m     32\u001B[0m }\n\u001B[0;32m---> 34\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEEP_SEEK_MODEL_KEY\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# use tweaked config\u001B[39;49;00m\n\u001B[1;32m     38\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Hugging Face pipeline now works with the custom model\u001B[39;00m\n\u001B[1;32m     41\u001B[0m pipe \u001B[38;5;241m=\u001B[39m pipeline(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m, model\u001B[38;5;241m=\u001B[39mmodel, tokenizer\u001B[38;5;241m=\u001B[39mDEEP_SEEK_MODEL_KEY, trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Source/jkc/jims-ml-sandbox/.venv-keras2/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:559\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    557\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mregister(config\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m, model_class, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    558\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m add_generation_mixin_to_remote_model(model_class)\n\u001B[0;32m--> 559\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    560\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    561\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n",
      "File \u001B[0;32m~/Source/jkc/jims-ml-sandbox/.venv-keras2/lib/python3.10/site-packages/transformers/modeling_utils.py:262\u001B[0m, in \u001B[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    260\u001B[0m old_dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mget_default_dtype()\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 262\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    264\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_default_dtype(old_dtype)\n",
      "File \u001B[0;32m~/Source/jkc/jims-ml-sandbox/.venv-keras2/lib/python3.10/site-packages/transformers/modeling_utils.py:3698\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3695\u001B[0m     hf_quantizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3697\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 3698\u001B[0m     \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3699\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3700\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3701\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3702\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3703\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3704\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3705\u001B[0m     torch_dtype \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_torch_dtype(torch_dtype)\n\u001B[1;32m   3706\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_device_map(device_map)\n",
      "File \u001B[0;32m~/Source/jkc/jims-ml-sandbox/.venv-keras2/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:75\u001B[0m, in \u001B[0;36mBnb4BitHfQuantizer.validate_environment\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     72\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     73\u001B[0m     )\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_bitsandbytes_available():\n\u001B[0;32m---> 75\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     77\u001B[0m     )\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m validate_bnb_backend_availability\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001B[0;31mImportError\u001B[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "jims_keras2_sandbox",
   "language": "python",
   "display_name": "Jim's Keras 2 Sandbox"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
